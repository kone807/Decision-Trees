{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Implementation\n",
    "\n",
    "## Submitted by: Hardik Garg, part of CN DS+ML course\n",
    "\n",
    "Note to the grader: The optional part is also done (actual decision tree is implemented), which means that \"printing decision tree steps as in example\" and actual decision tree implementation\" are done together.\n",
    "\n",
    "In other words, calling the fit function on the decision_tree prints the levels as in example and trains the model, both at the same time.\n",
    "\n",
    "Then, PDFs for both - the iris dataset and the OR logic gate are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self, split_feature, majority_class):\n",
    "        \n",
    "        ## split_feature represents feature by which we split current node\n",
    "        self.split_feature = split_feature\n",
    "        \n",
    "        ## dictionary to hold <feature_value> - <child_node>\n",
    "        self.children = {}\n",
    "        \n",
    "        ## store the majority class corresponding to the current node\n",
    "        self.majority_class = majority_class\n",
    "        \n",
    "        ## index (for keeping track) of current node\n",
    "        self.index = -1\n",
    "        \n",
    "    def add_child(self, feature_value, child):\n",
    "        \n",
    "        self.children[feature_value] = child\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        ## root of our tree is initially None\n",
    "        self.root=None\n",
    "        \n",
    "    ## returns a dictionary corresponding to the freq of each class in y\n",
    "    def get_class_freq(self, y):\n",
    "        \n",
    "        freq_dict = {}\n",
    "        \n",
    "        for val in y:\n",
    "            \n",
    "            if val in freq_dict:\n",
    "                freq_dict[val]+=1\n",
    "            else:\n",
    "                freq_dict[val]=1\n",
    "        \n",
    "        return freq_dict\n",
    "       \n",
    "    ## returns entropy of given array\n",
    "    def entropy(self, y):\n",
    "        class_freq_dict = self.get_class_freq(y)\n",
    "        _entropy=0\n",
    "        total_num = len(y)\n",
    "        \n",
    "        for key in class_freq_dict:\n",
    "            \n",
    "            p = class_freq_dict[key]/total_num\n",
    "            _entropy += -p*math.log2(p)\n",
    "            \n",
    "        return _entropy\n",
    "    \n",
    "    ## function to return the gain ratio\n",
    "    def gain_ratio(self, x, y, feature):\n",
    "        \n",
    "        ## entropy before splitting\n",
    "        info_gain_parent = self.entropy(y)\n",
    "        \n",
    "        ## entropy, split_info after splitting is calculated now\n",
    "        info_gain_children = 0\n",
    "        split_info = 0\n",
    "        \n",
    "        ## extract unique x values\n",
    "        unique_values = set(x[:,feature])\n",
    "        \n",
    "        ## dataframe having x-y values\n",
    "        df = pd.DataFrame(x)\n",
    "        df[df.shape[1]] = y\n",
    "        \n",
    "        ## total number of all values of the feature (denominator)\n",
    "        total_num = df.shape[0]\n",
    "        for val in unique_values:\n",
    "            \n",
    "            df_curr = df[df[feature]==val]\n",
    "            \n",
    "            ## total entries in current val of feature (numerator)\n",
    "            curr_num = df_curr.shape[0]\n",
    "            \n",
    "            ## calling entropy on the last col of dataframe\n",
    "            ## we are taking weighted sum of entropies of all values of the feature\n",
    "            info_gain_children += (curr_num/total_num)*self.entropy(df_curr[df_curr.shape[1]-1])\n",
    "            \n",
    "            ## update split info\n",
    "            if curr_num != 0:\n",
    "                split_info += -(curr_num/total_num)*math.log2(curr_num/total_num)\n",
    "        \n",
    "        ## when split_info is 0, we have a pure node\n",
    "        if split_info == 0:\n",
    "            return math.inf\n",
    "        \n",
    "        info_gain = info_gain_parent - info_gain_children\n",
    "        return info_gain / split_info\n",
    "            \n",
    "            \n",
    "        \n",
    "    ## function to build tree, level denotes current depth, all_feature_names stored names of all features\n",
    "    ## (useful for printing)\n",
    "    ## this is a recursive function\n",
    "    ## returns the root of the decision tree\n",
    "    def build_decision_tree(self, x, y, features, level, all_classes, all_feature_names=np.array([f for f in df.columns])):\n",
    "        \n",
    "        ## base case\n",
    "        \n",
    "        ## if no features are left\n",
    "        if len(features)==0:\n",
    "            \n",
    "            majority_class = None\n",
    "            class_freq_dict = self.get_class_freq(y)\n",
    "            max_freq_count = -math.inf\n",
    "            \n",
    "            print(\"Level:\",level)\n",
    "            \n",
    "            for _class in all_classes:\n",
    "                \n",
    "                if _class in class_freq_dict:\n",
    "                    \n",
    "                    if class_freq_dict[_class] > max_freq_count:\n",
    "                        max_freq_count = class_freq_dict[_class]\n",
    "                        majority_class=_class\n",
    "                        \n",
    "                    print(\"Count of\", _class,\":\", class_freq_dict[_class])\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Count of\", _class,\":\",0) \n",
    "                    \n",
    "            print(\"Current Entropy: \", self.entropy(y))\n",
    "            print(\"Reached leaf node\")\n",
    "            print()\n",
    "            \n",
    "            ## leaf node can't be split further so there is no split_feature, but there is a majority_feature\n",
    "            return Node(None, majority_class)\n",
    "        \n",
    "        \n",
    "        ## if the node is pure\n",
    "        if len(set(y)) == 1:\n",
    "            \n",
    "            majority_class = None\n",
    "            \n",
    "            print(\"Level:\",level)\n",
    "            \n",
    "            for _class in all_classes:\n",
    "                \n",
    "                if _class in y:\n",
    "                    majority_class = _class\n",
    "                    print(\"Count of\", _class,\":\", len(y))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Count of\", _class,\":\",0) \n",
    "                    \n",
    "            print(\"Current Entropy: \", 0)\n",
    "            print(\"Reached leaf node\")\n",
    "            print()\n",
    "            \n",
    "            ## leaf node can't be split further so there is no split_feature, but there is a majority_feature\n",
    "            return Node(None, majority_class)\n",
    "        \n",
    "        \n",
    "        ## now we write the main part which is run when base case is not executed. \n",
    "        ## here we find the best_split_feature to split the current node (generating maximum information gain)\n",
    "        \n",
    "        best_split_feature = None\n",
    "        max_gain_ratio = -math.inf\n",
    "        \n",
    "        for feature in features:\n",
    "            \n",
    "            curr_gain_ratio = self.gain_ratio(x,y,feature)\n",
    "            \n",
    "            if curr_gain_ratio > max_gain_ratio:\n",
    "                \n",
    "                max_gain_ratio = curr_gain_ratio\n",
    "                best_split_feature = feature\n",
    "                \n",
    "                \n",
    "        ## printing details for current level\n",
    "        \n",
    "        ## finding majority_class in current y\n",
    "        class_freq_dict = self.get_class_freq(y)\n",
    "        max_freq_count = -math.inf\n",
    "        majority_class=None\n",
    "        \n",
    "        print(\"Level:\",level)\n",
    "            \n",
    "        for _class in all_classes:\n",
    "                \n",
    "            if _class in class_freq_dict:\n",
    "                    \n",
    "                if class_freq_dict[_class] > max_freq_count:\n",
    "                    max_freq_count = class_freq_dict[_class]\n",
    "                    majority_class=_class\n",
    "                        \n",
    "                print(\"Count of\", _class,\":\", class_freq_dict[_class])\n",
    "                    \n",
    "            else:\n",
    "                print(\"Count of\", _class,\":\",0) \n",
    "                    \n",
    "        print(\"Current Entropy: \", self.entropy(y))\n",
    "        print(\"Splitting on feature\", all_feature_names[best_split_feature], \"with gain ratio:\", max_gain_ratio)\n",
    "        print()\n",
    "        \n",
    "        ## preparing for recursive calls\n",
    "            \n",
    "        ## getting set of all unique x values\n",
    "        unique_values = set(x[:,best_split_feature])\n",
    "        \n",
    "        df = pd.DataFrame(x)\n",
    "        ## append y value in the end \n",
    "        df[df.shape[1]]=y\n",
    "        \n",
    "        ## creating proper tree node (root)\n",
    "        root_node = Node(best_split_feature, majority_class)\n",
    "        \n",
    "        ## remove best_split_feature \n",
    "        idx = features.index(best_split_feature)\n",
    "        features.remove(best_split_feature)\n",
    "        #idx = np.where(features==best_split_feature)\n",
    "        #features = np.delete(features, idx)\n",
    "        \n",
    "        ## calling recursion on all children of root_node\n",
    "        \n",
    "        for val in unique_values:\n",
    "            \n",
    "            ## dataFrame corresponding to current unique value, using boolean indexing\n",
    "            df_curr = df[df[best_split_feature]==val]\n",
    "            \n",
    "            ## defining x_ and y_ for the child\n",
    "            x_ = df_curr.iloc[:,:df_curr.shape[1]-1].values\n",
    "            y_ = df_curr.iloc[:, df_curr.shape[1]-1].values\n",
    "            \n",
    "            child_node = self.build_decision_tree(x_, y_, features, level+1, all_classes)\n",
    "            root_node.add_child(val, child_node)\n",
    "            \n",
    "        ## re-add the removed feautre\n",
    "        features.insert(idx, best_split_feature)\n",
    "        #features.insert(best_split_feature, idx)\n",
    "        #np.insert(features, idx, best_split_feature)\n",
    "        \n",
    "        ## return root_node\n",
    "        return root_node\n",
    "        \n",
    "        \n",
    "    ## function to fit dataset to decision tree\n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        ## ex - for iris, x has 4 features which is len(x[0])\n",
    "        features = [i for i in range(len(x[0]))]\n",
    "        \n",
    "        ## all_classes includes the \"names\" of all output classes\n",
    "        all_classes = set(y)\n",
    "        level=0\n",
    "        \n",
    "        ## fitting the model\n",
    "        self.root = self.build_decision_tree(x,y,features,level,all_classes)\n",
    "            \n",
    "    ## recursive function to reach the result for a single training point\n",
    "    ## returns the majority class accordingly\n",
    "    \n",
    "    def single_point_predict(self, xi, node):\n",
    "        \n",
    "        ## base case (when we are at a leaf node)\n",
    "        if len(node.children) == 0:\n",
    "            return node.majority_class\n",
    "        \n",
    "        ## value of xi corresponding to the split feature \n",
    "        split_feature_value=xi[node.split_feature]\n",
    "        \n",
    "        ## if current class is a pure class\n",
    "        if split_feature_value not in node.children:\n",
    "            return node.majority_class\n",
    "        \n",
    "        ## else call recursion\n",
    "        return self.single_point_predict(xi, node.children[split_feature_value])\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        y = np.array([0 for i in range(len(x))])\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            y[i] = self.single_point_predict(x[i], self.root)\n",
    "        \n",
    "        return y\n",
    "       \n",
    "    def score(self, x, y):\n",
    "        \n",
    "        ## returns the mean accuracy\n",
    "        y_pred = self.predict(x)\n",
    "        count=0\n",
    "        \n",
    "        for i in range(len(y)):\n",
    "            \n",
    "            if y[i] == y_pred[i]:\n",
    "                count+=1\n",
    "                \n",
    "        return count/len(y)\n",
    "        \n",
    "    def export_tree_pdf(self, filename=None):\n",
    "        \n",
    "        from collections import deque\n",
    "        \n",
    "        dot_data = '''digraph Tree {node [shape=box] ;'''\n",
    "        \n",
    "        queue = deque()\n",
    "        \n",
    "        r = self.root\n",
    "        queue.append(r)\n",
    "        count = 0\n",
    "        if r.index == -1:\n",
    "            r.index = count\n",
    "        \n",
    "        dot_data = dot_data + \"\\n{} [label=\\\"Feature to split upon : X[{}]\\\\nOutput at this node : {}\\\" ];\".format(count,r.split_feature,r.majority_class) \n",
    "        \n",
    "        # Doing LEVEL ORDER traversal in the tree (using a queue)\n",
    "        while len(queue) != 0 :\n",
    "            node = queue.popleft()\n",
    "            for i in node.children:\n",
    "                count+=1\n",
    "                if(node.children[i].index==-1):\n",
    "                    node.children[i].index = count\n",
    "                \n",
    "                # Creating child node\n",
    "                dot_data = dot_data + \"\\n{} [label=\\\"Feature to split upon : X[{}]\\\\nMajority_Class at this node : {}\\\" ];\".format(node.children[i].index,node.children[i].split_feature,node.children[i].majority_class) \n",
    "                # Connecting parent node with child\n",
    "                dot_data = dot_data + \"\\n{} -> {} [ headlabel=\\\"Feature value = {}\\\"]; \".format(node.index,node.children[i].index,i)\n",
    "                # Adding child node to queue\n",
    "                queue.append(node.children[i])\n",
    "        \n",
    "        dot_data = dot_data + \"\\n}\"\n",
    "\n",
    "        if filename != None:    \n",
    "            graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf(filename)    \n",
    "        \n",
    "        return dot_data\n",
    "                    \n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some data preprocessing\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(iris.data)\n",
    "\n",
    "## sepal_length, sepal_width, petal_length, petal_width\n",
    "df.columns = [\"sl\", \"sw\", \"pl\", \"pw\"]\n",
    "\n",
    "## function to convert values in each field to discrete values based on a threshold\n",
    "## we take 4 boundaries as discussed in lecture video around 3 points-\n",
    "## 1. (min_val + mean)/2 => this is less than mean\n",
    "## 2. mean_value => this is equal to mean\n",
    "## 3. (max_value + mean)/2 => this is greater than mean\n",
    "\n",
    "def convert_to_discrete(value, *args):\n",
    "    \n",
    "    if value < args[0]:\n",
    "        return 0\n",
    "    if value < args[1]:\n",
    "        return 1\n",
    "    if value < args[2]:\n",
    "        return 2\n",
    "    return 3\n",
    "    \n",
    "for col in df.columns:\n",
    "    \n",
    "    mean_val = df[col].mean()\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    \n",
    "    ## array holding points around which we split continuous values into labels\n",
    "    threshold_arr = np.array([(min_val+mean_val)/2, mean_val, (max_val+mean_val)/2])\n",
    "    \n",
    "    df[col] = df[col].apply(convert_to_discrete, args=(threshold_arr[0], threshold_arr[1], threshold_arr[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing the decision tree\n",
    "\n",
    "x = df.values\n",
    "y = iris.target\n",
    "features = set(df.columns)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 0\n",
      "Count of 0 : 37\n",
      "Count of 1 : 34\n",
      "Count of 2 : 41\n",
      "Current Entropy:  1.5807197138422104\n",
      "Splitting on feature pl with gain ratio: 0.6802317879850113\n",
      "\n",
      "Level: 1\n",
      "Count of 0 : 37\n",
      "Count of 1 : 0\n",
      "Count of 2 : 0\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 1\n",
      "Count of 0 : 0\n",
      "Count of 1 : 6\n",
      "Count of 2 : 0\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 1\n",
      "Count of 0 : 0\n",
      "Count of 1 : 28\n",
      "Count of 2 : 17\n",
      "Current Entropy:  0.9564574047992594\n",
      "Splitting on feature pw with gain ratio: 0.35184707702123014\n",
      "\n",
      "Level: 2\n",
      "Count of 0 : 0\n",
      "Count of 1 : 3\n",
      "Count of 2 : 0\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 2\n",
      "Count of 0 : 0\n",
      "Count of 1 : 25\n",
      "Count of 2 : 8\n",
      "Current Entropy:  0.7990485210442682\n",
      "Splitting on feature sl with gain ratio: 0.14662562110534788\n",
      "\n",
      "Level: 3\n",
      "Count of 0 : 0\n",
      "Count of 1 : 0\n",
      "Count of 2 : 1\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 3\n",
      "Count of 0 : 0\n",
      "Count of 1 : 7\n",
      "Count of 2 : 0\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 3\n",
      "Count of 0 : 0\n",
      "Count of 1 : 16\n",
      "Count of 2 : 7\n",
      "Current Entropy:  0.8865408928220899\n",
      "Splitting on feature sw with gain ratio: 0.08767645528023384\n",
      "\n",
      "Level: 4\n",
      "Count of 0 : 0\n",
      "Count of 1 : 3\n",
      "Count of 2 : 1\n",
      "Current Entropy:  0.8112781244591328\n",
      "Reached leaf node\n",
      "\n",
      "Level: 4\n",
      "Count of 0 : 0\n",
      "Count of 1 : 9\n",
      "Count of 2 : 6\n",
      "Current Entropy:  0.9709505944546686\n",
      "Reached leaf node\n",
      "\n",
      "Level: 4\n",
      "Count of 0 : 0\n",
      "Count of 1 : 4\n",
      "Count of 2 : 0\n",
      "Current Entropy:  0.0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 3\n",
      "Count of 0 : 0\n",
      "Count of 1 : 2\n",
      "Count of 2 : 0\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 2\n",
      "Count of 0 : 0\n",
      "Count of 1 : 0\n",
      "Count of 2 : 9\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 1\n",
      "Count of 0 : 0\n",
      "Count of 1 : 0\n",
      "Count of 2 : 24\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'digraph Tree {node [shape=box] ;\\n0 [label=\"Feature to split upon : X[2]\\\\nOutput at this node : 2\" ];\\n1 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 0\" ];\\n0 -> 1 [ headlabel=\"Feature value = 0\"]; \\n2 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n0 -> 2 [ headlabel=\"Feature value = 1\"]; \\n3 [label=\"Feature to split upon : X[3]\\\\nMajority_Class at this node : 1\" ];\\n0 -> 3 [ headlabel=\"Feature value = 2\"]; \\n4 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 2\" ];\\n0 -> 4 [ headlabel=\"Feature value = 3\"]; \\n5 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n3 -> 5 [ headlabel=\"Feature value = 1\"]; \\n6 [label=\"Feature to split upon : X[0]\\\\nMajority_Class at this node : 1\" ];\\n3 -> 6 [ headlabel=\"Feature value = 2\"]; \\n7 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 2\" ];\\n3 -> 7 [ headlabel=\"Feature value = 3\"]; \\n8 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 2\" ];\\n6 -> 8 [ headlabel=\"Feature value = 0\"]; \\n9 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n6 -> 9 [ headlabel=\"Feature value = 1\"]; \\n10 [label=\"Feature to split upon : X[1]\\\\nMajority_Class at this node : 1\" ];\\n6 -> 10 [ headlabel=\"Feature value = 2\"]; \\n11 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n6 -> 11 [ headlabel=\"Feature value = 3\"]; \\n12 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n10 -> 12 [ headlabel=\"Feature value = 0\"]; \\n13 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n10 -> 13 [ headlabel=\"Feature value = 1\"]; \\n14 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n10 -> 14 [ headlabel=\"Feature value = 2\"]; \\n}'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build tree\n",
    "\n",
    "dec_tree = Decision_Tree()\n",
    "\n",
    "## fit\n",
    "dec_tree.fit(x_train, y_train)\n",
    "\n",
    "## saving pdf\n",
    "dec_tree.export_tree_pdf(filename = \"iris_tree.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9375\n",
      "Test score:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score: \",dec_tree.score(x_train, y_train))\n",
    "print(\"Test score: \",dec_tree.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 0\n",
      "Count of 0 : 1\n",
      "Count of 1 : 3\n",
      "Current Entropy:  0.8112781244591328\n",
      "Splitting on feature sl with gain ratio: 0.31127812445913283\n",
      "\n",
      "Level: 1\n",
      "Count of 0 : 1\n",
      "Count of 1 : 1\n",
      "Current Entropy:  1.0\n",
      "Splitting on feature sw with gain ratio: 1.0\n",
      "\n",
      "Level: 2\n",
      "Count of 0 : 1\n",
      "Count of 1 : 0\n",
      "Current Entropy:  0.0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 2\n",
      "Count of 0 : 0\n",
      "Count of 1 : 1\n",
      "Current Entropy:  0.0\n",
      "Reached leaf node\n",
      "\n",
      "Level: 1\n",
      "Count of 0 : 0\n",
      "Count of 1 : 2\n",
      "Current Entropy:  0\n",
      "Reached leaf node\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'digraph Tree {node [shape=box] ;\\n0 [label=\"Feature to split upon : X[0]\\\\nOutput at this node : 1\" ];\\n1 [label=\"Feature to split upon : X[1]\\\\nMajority_Class at this node : 0\" ];\\n0 -> 1 [ headlabel=\"Feature value = 0\"]; \\n2 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n0 -> 2 [ headlabel=\"Feature value = 1\"]; \\n3 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 0\" ];\\n1 -> 3 [ headlabel=\"Feature value = 0\"]; \\n4 [label=\"Feature to split upon : X[None]\\\\nMajority_Class at this node : 1\" ];\\n1 -> 4 [ headlabel=\"Feature value = 1\"]; \\n}'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generating pdf for OR gate\n",
    "\n",
    "dec_tree = Decision_Tree()\n",
    "\n",
    "x_or = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_or = np.array([0,1,1,1])\n",
    "\n",
    "dec_tree.fit(x_or, y_or)\n",
    "y_pred = dec_tree.predict(x_or)\n",
    "\n",
    "## saving pdf\n",
    "dec_tree.export_tree_pdf(filename = \"or_tree.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
